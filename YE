Bucket 1: Deliverables and Project-Specific Goals

1. Enabling Lakshman to be Self-Reliant
I observed significant progress in Lakshman’s ability to work independently. His communication skills have improved remarkably, enabling him to convey problems in clear and concise terms. This clarity has enhanced collaboration with our onsite peers, who now understand issues more effectively. Lakshman has also gained a strong command of the ezFlow source code, positioning himself as the primary point of contact for knowledge transfer to new team members. With these advancements, I am confident he will continue to excel in his contributions.

2. Livy Ecosystem for Auto-Sourcing
I spearheaded the development of a Livy-based ecosystem that replaced the traditional SSH-based method of invoking ezFlow from the RDD portal. This transformation introduced an API-driven approach, overcoming significant challenges due to Livy’s novelty within the bank and the lack of an established data platform around it. Despite these hurdles, I delivered a resilient Livy framework capable of handling Spark requests for both the RDD portal and the Auto-Sourcing ecosystem.

3. Real-Time Data Quality Monitoring with the Qlert Daemon
The existing Qlert system, designed by the WCRV team, operated as a batch process, delaying data quality (DQ) results until the end of BAU batch processing. Recognizing the need for real-time DQ insights, I developed a Java-based daemon. This solution continuously monitored Oracle’s queueing system and executed DQ checks as tables were loaded, significantly enhancing the efficiency of DQ reporting.

4. High-Performance Text File Processor
The legacy text file processor struggled to handle high data volumes due to its reliance on outdated APIs. Addressing this challenge, I conducted multiple design iterations to modernize the processor while retaining 99% of its original features. The revamped code base now operates efficiently, meeting the demands of high-volume data processing.

5. Resolving CVEs in ezFlow
I addressed critical vulnerabilities in ezFlow’s code base that were flagged during security scans. As part of the RISE deliverables, I replaced vulnerable libraries, ensuring compliance with security standards and enhancing system resilience.

6. API Integration for ezFlow Invocation
To support a test playground in the RDD portal, I developed a backend API in Node.js. This API enabled users to test specific workflows via the UI by connecting to an edge node and executing scripts. The feature has been seamlessly integrated into the portal’s backend codebase, enhancing user testing capabilities.

7. RDAR Marketplace POC
I contributed to a proof-of-concept (POC) initiative to migrate risk data aggregation (RDAR) to the DataConnect platform. This involved automating three marketplace reports entirely within DataConnect and Falcon. My role spanned data modeling, sourcing transformations, and converting existing SQL scripts into the DataConnect language, ensuring a smooth and efficient transition.

Bucket 2: Self-Learning Goals

Advanced Certification from NCFM
Achieving the advanced module certification in options from NCFM was a longstanding goal. This certification tested my analytical and problem-solving skills, demanding precision to the last decimal point. The rigorous preparation and successful completion have significantly enhanced my expertise.

Bucket 3: Stretch Assignment Goals

1. Increased Learning Sessions for TFG
This year, I conducted over 15 hours of training sessions focused on Spark, Scala, and data analytics with Python. These sessions provided an opportunity to connect with enthusiastic learners, motivating me to consistently raise the bar for content quality and engagement.

2. Elevate Cohort Training Group
As a trainer for a cohort of over 10 learners, I delivered an interactive Scala training program. Unlike traditional TFG sessions, this program included hands-on practical exercises and assignments, allowing participants to assess and improve their skills effectively.

3. ElevateU Hadoop Training
In support of the BCBS data migration effort, transitioning from Exadata to Hadoop, I conducted training sessions on Spark and ezFlow. These sessions were instrumental in equipping the team with the necessary skills to adapt to the new ecosystem.

4. Finance-Based Quiz Competition
As part of the DLF fest, I organized a finance-themed quiz competition. This initiative provided an engaging platform for participants to test their financial acumen while fostering a spirit of healthy competition.

Bucket 4: Team-Specific Goals

ERFT Innovation Challenge
I led a team of five to participate in the ERFT Innovation Challenge. We developed a proof-of-concept (POC) for a lineage-gathering platform using existing tools within the bank’s tech stack. By applying a real-world production use case, we demonstrated how our platform could integrate seamlessly. The POC was well-received, with particular appreciation for its clean presentation and user-friendly interface.

